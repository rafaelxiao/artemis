{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "import time\n",
    "import random\n",
    "import platform\n",
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import itertools\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "from lxml import etree\n",
    "from selenium import webdriver"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class CrawlDoubanTV():\n",
    "    \n",
    "    user_agent = [\n",
    "        'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_12_6) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/61.0.3163.100 Safari/537.36',\n",
    "        'Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/39.0.2171.95 Safari/537.36 OPR/26.0.1656.60',\n",
    "        'Mozilla/5.0 (Windows NT 5.1; U; en; rv:1.8.1) Gecko/20061208 Firefox/2.0.0 Opera 9.50',\n",
    "        'Mozilla/4.0 (compatible; MSIE 6.0; Windows NT 5.1; en) Opera 9.50',\n",
    "        'Mozilla/5.0 (Windows NT 6.1; WOW64; rv:34.0) Gecko/20100101 Firefox/34.0',\n",
    "        'Mozilla/5.0 (X11; U; Linux x86_64; zh-CN; rv:1.9.2.10) Gecko/20100922 Ubuntu/10.10 (maverick) Firefox/3.6.10',\n",
    "        'Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/534.57.2 (KHTML, like Gecko) Version/5.1.7 Safari/534.57.2',\n",
    "        'Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/39.0.2171.71 Safari/537.36',\n",
    "        'Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.11 (KHTML, like Gecko) Chrome/23.0.1271.64 Safari/537.11',\n",
    "        'Mozilla/5.0 (Windows; U; Windows NT 6.1; en-US) AppleWebKit/534.16 (KHTML, like Gecko) Chrome/10.0.648.133 Safari/534.16',\n",
    "        'Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/30.0.1599.101 Safari/537.36',\n",
    "        'Mozilla/5.0 (Windows NT 6.1; WOW64; Trident/7.0; rv:11.0) like Gecko',\n",
    "        'Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/537.1 (KHTML, like Gecko) Chrome/21.0.1180.71 Safari/537.1 LBBROWSER',\n",
    "        'Mozilla/5.0 (compatible; MSIE 9.0; Windows NT 6.1; WOW64; Trident/5.0; SLCC2; .NET CLR 2.0.50727; .NET CLR 3.5.30729; .NET CLR 3.0.30729; Media Center PC 6.0; .NET4.0C; .NET4.0E; LBBROWSER)',\n",
    "        'Mozilla/5.0 (compatible; MSIE 9.0; Windows NT 6.1; WOW64; Trident/5.0; SLCC2; .NET CLR 2.0.50727; .NET CLR 3.5.30729; .NET CLR 3.0.30729; Media Center PC 6.0; .NET4.0C; .NET4.0E; QQBrowser/7.0.3698.400)',\n",
    "        'Mozilla/4.0 (compatible; MSIE 6.0; Windows NT 5.1; SV1; QQDownload 732; .NET4.0C; .NET4.0E)',\n",
    "        'Mozilla/5.0 (Windows NT 5.1) AppleWebKit/535.11 (KHTML, like Gecko) Chrome/17.0.963.84 Safari/535.11 SE 2.X MetaSr 1.0',\n",
    "        'Mozilla/4.0 (compatible; MSIE 7.0; Windows NT 5.1; Trident/4.0; SV1; QQDownload 732; .NET4.0C; .NET4.0E; SE 2.X MetaSr 1.0)',\n",
    "    ]\n",
    "    \n",
    "    def __init__(self, scrolls=25, headless=True, sleep=0.2, discover=None):\n",
    "        self.scrolls = scrolls\n",
    "        self.options = webdriver.ChromeOptions()\n",
    "        self.ids = []\n",
    "        self.sleep = sleep\n",
    "        self.discover = discover\n",
    "        self.result = {}\n",
    "        if headless == True:\n",
    "            self.options.add_argument('--headless')\n",
    "            self.options.add_argument('--disable-gpu')\n",
    "            \n",
    "    def run(self):\n",
    "        self._crawl_main()\n",
    "        self._crawl_detail()\n",
    "    \n",
    "    def _crawl_main(self):\n",
    "        if platform.system() == 'Windows':\n",
    "            driver = webdriver.Chrome(\"C:\\Program Files (x86)\\Google\\Chrome\\Application\\chromedriver.exe\",\n",
    "                                                  options=self.options)\n",
    "\n",
    "        if platform.system() == 'Darwin':\n",
    "            driver = webdriver.Chrome(\"/Users/Lunar/chromedriver/chromedriver\",\n",
    "                                                  options=self.options)\n",
    "        \n",
    "        print('Crawling main...')\n",
    "        driver.get('https://movie.douban.com/tv/')\n",
    "        time.sleep(1)\n",
    "        \n",
    "        driver.find_element_by_xpath(\"//input[@value='国产剧']/..\").click()\n",
    "        time.sleep(1)\n",
    "        \n",
    "        for i in range(self.scrolls):\n",
    "            try:\n",
    "                if driver.find_element_by_xpath(\"//a[text()='加载更多']\").is_displayed():\n",
    "                    driver.find_element_by_xpath(\"//a[text()='加载更多']\").click()\n",
    "                    time.sleep(1+random.random())\n",
    "            except:\n",
    "                time.sleep(3)\n",
    "            print('{} / {}'.format(i+1, self.scrolls), end='\\r')\n",
    "            \n",
    "        f = etree.HTML(driver.page_source)\n",
    "        self.ids += f.xpath('//div[@class=\\'cover-wp\\']/@data-id')\n",
    "            \n",
    "        print(len(self.ids), self.ids[:5])\n",
    "        print('Main crawled')\n",
    "        \n",
    "    def _generate_tv_pages(self, id_):\n",
    "        head = 'https://movie.douban.com/subject/'\n",
    "        tail = random.choice(['/?tag=国产剧&from=gaia_video', '/?from=subject-page'])\n",
    "        return head + str(id_) + tail\n",
    "    \n",
    "    def _crawl_detail(self):\n",
    "        print('Crawling details...')\n",
    "        if self.discover:\n",
    "            iters = self.discover\n",
    "        else:\n",
    "            iters = len(self.ids)\n",
    "        for _ in tqdm(range(iters)):\n",
    "            try:\n",
    "                if len(self.ids) > 0:\n",
    "                    id_ = random.choice(self.ids)\n",
    "                    url = self._generate_tv_pages(id_)\n",
    "                    headers = {'user-agent': random.choice(self.user_agent)}\n",
    "                    text = requests.get(url=url, headers=headers).text\n",
    "                    if self.discover:\n",
    "                        self._discover_detail(text)\n",
    "                    self._parse_details(text)\n",
    "                    self.ids.remove(id_)\n",
    "                    time.sleep(self.sleep + random.random() * self.sleep)\n",
    "                else: break\n",
    "            except: pass\n",
    "        print('Done')\n",
    "    \n",
    "    # discvoer url from detail page and update the ids to crawl\n",
    "    def _discover_detail(self, text):\n",
    "        f = etree.HTML(text)\n",
    "        found = f.xpath(\"//div[@id='recommendations']//dt/a[@data-moreurl-dict]/@data-moreurl-dict\")\n",
    "        found =  [eval(i)['subject_id'] for i in found]\n",
    "        found = [i for i in found if i not in self.result]\n",
    "        self.ids += found\n",
    "        self.ids = list(set(self.ids))\n",
    "        \n",
    "    def _parse_details(self, text):\n",
    "        f = etree.HTML(text)\n",
    "        content = json.loads(f.xpath(\"//script[@type='application/ld+json']\")[0].text)\n",
    "        \n",
    "        id_ = content['url'].split('/')[-2]\n",
    "        parsed = {}\n",
    "        parsed['name'] = content['name']\n",
    "        parsed['date_published'] = content['datePublished']\n",
    "        parsed['director'] = {i['url'].split('/')[-2]: i['name'].split(' ')[0] for i in content['director']}\n",
    "        parsed['author'] = {i['url'].split('/')[-2]: i['name'].split(' ')[0] for i in content['author']}\n",
    "\n",
    "        hold =[(i['url'].split('/')[-2], i['name'].split(' ')[0]) for i in content['actor']]\n",
    "        parsed['main_actor'] = {id_: name for id_, name in hold[:5]}\n",
    "        parsed['support_actor'] = {id_: name for id_, name in hold[5:]}\n",
    "\n",
    "        parsed['genre'] = content['genre']\n",
    "        parsed['duration'] = content['duration']\n",
    "        parsed['rating'] = content['aggregateRating']['ratingValue']\n",
    "        parsed['rating_count'] = content['aggregateRating']['ratingCount']\n",
    "        parsed['rating_best'] = content['aggregateRating']['bestRating']\n",
    "        parsed['rating_worst'] = content['aggregateRating']['worstRating']\n",
    "\n",
    "        parsed['location'] = f.xpath(\"//div[@id='info']//span[text()='制片国家/地区:']\")[0].tail.strip()\n",
    "        parsed['num_episode'] = f.xpath(\"//div[@id='info']//span[text()='集数:']\")[0].tail.strip()\n",
    "                \n",
    "        self.result[id_] = parsed\n",
    "\n",
    "\n",
    "\n",
    "def clean(dic):\n",
    "    id_to_pop = []\n",
    "    for i in dic:\n",
    "        if (len(dic[i]['author']) == 0 or len(dic[i]['director']) == 0) or ((\n",
    "            len(dic[i]['main_actor']) == 0 or len(dic[i]['support_actor']) == 0) or \n",
    "            dic[i]['location'] != '中国大陆'):\n",
    "            id_to_pop.append(i)\n",
    "    for i in id_to_pop:\n",
    "        dic.pop(i)\n",
    "    return dic\n",
    "\n",
    "def get_true_score(dic, typ='main_actor'):\n",
    "    char_dic = get_mapping(dic)\n",
    "    char_list = list(char_dic.keys())\n",
    "    tv_list = list(dic.keys())\n",
    "    rating_list = [float(dic[i]['rating']) for i in tv_list]\n",
    "    df = pd.DataFrame(index = tv_list, columns=char_list).fillna(0)\n",
    "    for i in tqdm(tv_list):\n",
    "        for j in char_list:\n",
    "            if j in dic[i][typ]:\n",
    "                df.loc[i, j] = 1\n",
    "    count = df.sum(1).values\n",
    "    df = df / count[:, None]\n",
    "    scores = np.linalg.pinv(df.values) @ np.array(rating_list).reshape(-1, 1)\n",
    "    scores = {i: j for (i, j) in zip(char_list, scores)}\n",
    "    count = {i: j for (i, j) in zip(char_list, count)}\n",
    "    return scores, count\n",
    "\n",
    "def get_avg_score(dic, typ='main_actor'):\n",
    "    char_dic = get_mapping(dic)\n",
    "    char_list = list(char_dic.keys())\n",
    "    tv_list = list(dic.keys())\n",
    "    rating_list = [float(dic[i]['rating']) for i in tv_list]\n",
    "    df = pd.DataFrame(index = tv_list, columns=char_list)\n",
    "    for i in tqdm(tv_list):\n",
    "        for j in char_list:\n",
    "            if j in dic[i][typ]:\n",
    "                df.loc[i, j] = float(dic[i]['rating'])\n",
    "    df['score'] = df.mean(1)\n",
    "    return {i: j for (i, j) in zip(char_list, df['score'])}\n",
    "\n",
    "def get_mapping(dic):\n",
    "    char_dic= {}\n",
    "    for i in dic:\n",
    "        char_dic = dict(char_dic, **dic[i]['author'])\n",
    "        char_dic = dict(char_dic, **dic[i]['director'])\n",
    "        char_dic = dict(char_dic, **dic[i]['main_actor'])\n",
    "        char_dic = dict(char_dic, **dic[i]['support_actor'])\n",
    "    return char_dic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Crawling main...\n",
      "400/ 25 ['27195020', '26794191', '26660368', '33413766', '30191989']\n",
      "Main crawled\n",
      "Crawling details...\n",
      "\n",
      "Done\n"
     ]
    }
   ],
   "source": [
    "crawl = CrawlDoubanTV(scrolls=25, discover=1500, sleep=0.3)\n",
    "crawl.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num of shows crawed: 355\n",
      "Num of chars crawed: 5986\n"
     ]
    }
   ],
   "source": [
    "mapping = get_mapping(crawl.result)\n",
    "print('Num of shows crawed: %s'%len(crawl.result))\n",
    "print('Num of chars crawed: %s'%len(mapping))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dic = crawl.result.copy()\n",
    "dic = clean(dic)\n",
    "\n",
    "main_avg = get_avg_score(clean(dic), typ='main_actor')\n",
    "main_true, main_count = get_true_score(clean(dic), typ='main_actor')\n",
    "support_avg = get_avg_score(clean(dic), typ='support_actor')\n",
    "support_true, support_count = get_true_score(clean(dic), typ='support_actor')\n",
    "\n",
    "\n",
    "df = pd.DataFrame(main_avg, index=['main_avg']).T.join(\n",
    "       pd.DataFrame(main_true, index=['main_true']).T).join(\n",
    "       pd.DataFrame(support_avg, index=['support_avg']).T).join(\n",
    "       pd.DataFrame(support_true, index=['support_true']).T).join(\n",
    "       pd.DataFrame(main_count, index=['main_count']).T).join(\n",
    "       pd.DataFrame(support_count, index=['support_count']).T)\n",
    "\n",
    "df['true_score'] = np.round(df['main_true'] * 0.7 + df['support_true'] * 0.3, 2)\n",
    "df.loc[df['true_score'] > 10, 'true_score'] = 10\n",
    "df.loc[df['true_score'] < 0, 'true_score'] = 0\n",
    "df['true_score'] = np.abs(df['true_score'])\n",
    "df['avg_score'] = np.round(df['main_avg'] * 0.7 + df['support_avg'] * 0.3, 2)\n",
    "df['total'] = df['main_count'] + df['support_count']\n",
    "df['main_pct'] = df['main_count'] / df['total']\n",
    "df['diff'] = df['true_score'] - df['avg_score']\n",
    "        \n",
    "df = df.drop(['main_avg', 'main_true', 'support_avg', 'support_true', 'main_count', 'support_count'], axis=1)\n",
    "df = df.rename(index=mapping)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df.to_excel('res.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  },
  "widgets": {
   "state": {
    "0e89de73c1fe4c43aeb6b59adf1cbf52": {
     "views": [
      {
       "cell_index": 2
      }
     ]
    },
    "39a0a97c8ba5421797b8af80ae3794ef": {
     "views": [
      {
       "cell_index": 4
      }
     ]
    },
    "959d09b92a0645c19606d1b1632aac0d": {
     "views": [
      {
       "cell_index": 4
      }
     ]
    },
    "be40cd732396402abdd7638a6fa1e6ff": {
     "views": [
      {
       "cell_index": 4
      }
     ]
    },
    "fa135f61aaa24493afe59f7fb07a4ca5": {
     "views": [
      {
       "cell_index": 4
      }
     ]
    }
   },
   "version": "1.2.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
